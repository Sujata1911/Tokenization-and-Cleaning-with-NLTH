# Tokenization-and-Cleaning-with-NLTK
This repository contains an efficient way to use NLTK for processing: Split into Sentences, 
Split into words,
Filtering out punctuations,
Filtering out stopwords,
Pipeline and,
stem words.
Download the raw file from http://www.gutenberg.org/cache/epub/5200/pg5200.txt before starting the project This project is based on the blog https://machinelearningmastery.com/clean-text-machine-learning-python/ by Jason Brownlee
